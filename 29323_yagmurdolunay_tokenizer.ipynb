{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **My Tokenizer**\n",
        "\n",
        "In this assignment, you are asked to create your own word tokenizer without the help of external tokenizers. Steps to the assignment:\n",
        "1. Choose one of the corpora from nltk.corpus list given - assign it to corpus_name\n",
        "1. Create your tokenizer in the code block - tokenize the selected corpus into token_list\n",
        "1. Give the raw corpus text, corpus_raw, and the my_token_list to the evaluation block\n",
        "\n",
        "Only splitting on whitespace is not enough. At least try two other improvements on the tokenization. Please write sufficient comments to show your reasoning.\n",
        "\n",
        "## Rules\n",
        "### Allowed:\n",
        " - Choosing a top-down tokenizer or bottom-up tokenizer\n",
        " - Using regular expressions library (import re)\n",
        " - Adding additional coding blocks\n",
        " - Having an additional dataset if you are creating a bottom-up tokenizer but you need to be able to run the code standalone.\n",
        "\n",
        "### Not allowed:\n",
        " - Using tokenizer libraries such as nltk.tokenize, or any other external libraries to tokenize.\n",
        " - Changing the contents of the evaluation block at the end of the notebook.\n",
        "\n",
        "## Assignment Report\n",
        "Please write a short assignment report at the end of the notebook (max 500 words). Please include all of the following points in the report:\n",
        " - Corpus name and the selection reason\n",
        " - Design of the tokenizer and reasoning\n",
        " - Challenges you have faced while writing the tokenizer and challenges with the specific corpus\n",
        " - Limitations of your approach\n",
        " - Possible improvements to the system\n",
        "\n",
        "## Grading\n",
        "You will be graded with the following criteria:\n",
        " - running complete code (0.5),\n",
        " - tokenizer algorithm (2),\n",
        " - clear commenting (0.5),\n",
        " - evaluation score - comparison with nltk word tokenizer (at most 1 point),\n",
        " - assignment report (1).\n",
        "\n",
        "## Submission\n",
        "\n",
        "Submission will be made to SUCourse. Please submit your file using the following naming convention.\n",
        "\n",
        "\n",
        "`studentid_studentname_tokenizer.ipynb Â - ex. 26744_aysegulrana_tokenizer.ipynb`\n",
        "\n",
        "\n",
        "**Deadline is October 22nd, 5pm.**"
      ],
      "metadata": {
        "id": "1TXP5nalr9ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "UnU-BEP4TNOG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_tokenizer(corpus_raw):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus to be tokenized\n",
        "    rtype: list\n",
        "    return: a list of tokens extracted from the corpus_raw\n",
        "    '''\n",
        "\n",
        "    # write your tokenizer here and apply to corpus_raw. Return the resulting token_list.\n",
        "    # you are NOT allowed to use external tokenizers such as word_tokenize from nltk.\n",
        "    # Only splitting on whitespace is not enough. At least try two other improvements on the tokenization.\n",
        "\n",
        "\n",
        "    token_list =[]\n",
        "    # Convert text to lowercase to handle case insensitivity\n",
        "    corpus_raw = corpus_raw.lower()\n",
        "\n",
        "    # firstly  handle URLs and emails\n",
        "    #https?: Matches \"http\" or \"https\".\n",
        "    #://: Matches the literal characters \"://\"\n",
        "    #[^\\s]+: Matches any sequence of characters that are not whitespace ([^\\s])\n",
        "    url_tokens = re.findall(r'https?://[^\\s]+|www\\.[^\\s]+|\\b[\\w.-]+?@\\w+?\\.\\w+?\\b', corpus_raw)\n",
        "\n",
        "    for token in url_tokens:\n",
        "        corpus_raw = corpus_raw.replace(token, '')  # Remove url tokens from original text\n",
        "\n",
        "    #handle emojis\n",
        "    '''[:;=8xX]: Matches any character that could start an emotion, like :, ;, =, 8, or X.\n",
        "        [-^o*]?: Optionally matches the nose of the emoton (characters like -, ^, o, or *).\n",
        "        [\\)\\(dDpP/\\|O3]: Matches the face part of the emotion (like ), (, d, D, P, etc.).'''\n",
        "    emotions = re.findall(r'[:;=8xX][-^o*]?[\\)\\(dDpP/\\|O3]', corpus_raw)\n",
        "    #for the hastags\n",
        "    hashtags = re.findall(r'#\\w+', corpus_raw)\n",
        "    #for the mentions ex:@yagmurdolunay\n",
        "    mentions = re.findall(r'@\\w+', corpus_raw)\n",
        "    #normalize punctuations ex: !!!!-> !\n",
        "    normalized_punctuations=re.sub(r'([!?]){2,}', r'\\1', corpus_raw)\n",
        "    #tokenize the remaining text (split by spaces, keep punctuation)\n",
        "    token_list = re.findall(r'\\w+|[^\\w\\s]', corpus_raw)  # Splits words but keeps punctuation as separate tokens\n",
        "\n",
        "    # Add back special tokens (URLs, emails, emoticons)\n",
        "    token_list.extend(url_tokens + emotions + hashtags+mentions)\n",
        "\n",
        "    return token_list"
      ],
      "metadata": {
        "id": "-jYyH_qz3Lii"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are allowed to add code blocks above to use for your tokenizer or evaluate it.\n",
        "\n"
      ],
      "metadata": {
        "id": "5HP0awlu3jci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#main code to run your tokenizer.\n",
        "\n",
        "#import your libraries here\n",
        "import nltk\n",
        "from nltk.corpus import webtext\n",
        "import re\n",
        "\n",
        "#select the corpus name from the list below\n",
        "#gutenberg, webtext, reuters, product_reviews_2\n",
        "\n",
        "corpus_name = 'webtext'\n",
        "\n",
        "#download the corpus and import it.\n",
        "nltk.download('webtext')\n",
        "\n",
        "#get the raw text output of the corpus to the corpus_raw variable.\n",
        "corpus_raw = webtext.raw()\n",
        "\n",
        "#call your tokenizer method\n",
        "my_tokenized_list = my_tokenizer(corpus_raw)\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZF4WJjKxZfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e64c58-43bc-4521-d056-a1c0aae027e0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Please do not touch the code below that will evaluate your tokenizer with the nltk word tokenizer. You will get zero points from evaluation if you do so."
      ],
      "metadata": {
        "id": "iuTDStwY36cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_score(set_a, set_b):\n",
        "    '''\n",
        "    type set_a: set\n",
        "    param set_a: The first set to be compared\n",
        "    type set_b: set\n",
        "    param set_b: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: similarity score with two sets using Jaccard similarity.\n",
        "    '''\n",
        "\n",
        "    jaccard_similarity = float(len(set_a.intersection(set_b)) / len(set_a.union(set_b)))\n",
        "\n",
        "    return jaccard_similarity"
      ],
      "metadata": {
        "id": "8txzw8Ag8ysD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import punkt\n",
        "\n",
        "def evaluation(corpus_raw, token_list):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus\n",
        "    type token_list: list\n",
        "    param token_list: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: comparison score with the given token list and the nltk tokenizer.\n",
        "    '''\n",
        "\n",
        "    #The comparison score only looks at the tokens but not the frequencies of the tokens.\n",
        "    #we assume case folding is already applied to the token_list\n",
        "    corpus_raw = corpus_raw.lower()\n",
        "    nltk_tokens = word_tokenize(corpus_raw, language='english')\n",
        "\n",
        "    score = similarity_score(set(token_list), set(nltk_tokens))\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "6wUTqReb36Hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13eae2e-82ad-4859-9b1f-598262526f06"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "\n",
        "eval_score = evaluation(corpus_raw, my_tokenized_list)\n",
        "\n",
        "print('The similarity score is {:.2f}'.format(eval_score))"
      ],
      "metadata": {
        "id": "6Vt_uSBF-NHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4003cd3f-a464-447a-d9f1-d58ba08a4ba1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity score is 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please write your report below using clear headlines with markdown syntax."
      ],
      "metadata": {
        "id": "keZ3UBqh4hgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report"
      ],
      "metadata": {
        "id": "Mt7azGprckaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Corpus Name: WebText**\n",
        "\n",
        "* *Selection Reason*\n",
        "\n",
        "    I chose the WebText corpora because it represents a modern, internet type of text, including blog posts, online chats, and conversations that are often found on platforms such as Reddit Twitter. In addition, I believe that human language is used more dynamically and often on the web and social media platforms, which makes WebText more relevant and interesting than other corporas such as Gutenberg or Reuters. These web-based resources reflect how language has evolved over time, especially in the context of informal conversations, making it ideal for tasks such as emotion analysis and web text mining.\n",
        "\n",
        "* *Design of the Tokenizer and Reasoning*\n",
        "\n",
        "    Tokenizer is designed to capture text elements in various informal and internet jargon found in the WebText corpus compilation. It starts by converting the text to lowercase to ensure case insensitivity. Special attention is paid to the protection of urls, emails, hashtags and mentions, which are necessary to understand the structure of online discourse.  Then, emoticons and emojis are detected using patterns that capture common internet emoticons. In addition, the token is used to indicate that exaggerated punctuation marks (\"!!!\") is reduced to a single sample, which helps to avoid noise. The remaining text is marked by dividing into spaces and punctuation marks, while keeping the punctuation marks as separate markers when relevant.\n",
        "* *Challenges Faced*\n",
        "\n",
        "    One of the primary challenges in tokenizing WebText was handling the wide variety of text types present in informal online conversations, such as URLs, social media handles, and informal grammar.  The most difficulty was detecting the characters that are used in the emoji formations and capturing them without breaking the structure of the text required careful use of regular expressions. Since Web Text contains a large number of irregular tokens, such as hashtags, mentions, and hyperlinks, a significant part of the development effort went into handling these tokens robustly.\n",
        "\n",
        "* *Limitations of my approach*\n",
        "\n",
        "    One of the biggest limitations of my tokenizer is that it lacks semantic understanding, as it works at the linguistic level, splitting and normalizing tokens without deeper language processing. It does not do lemmatization or rooting, that is, words are not reduced to root forms, which potentially affects tasks such as text classification. In addition, while the token builder captures common expressions, it struggles with more complex emoji patterns due to the lack of custom libraries. Finally, the normalization of punctuation marks (for example, \"!!!\" to  \"!\") and the lower case can eliminate important emphasis and context, which is very important in online communication.\n",
        "\n",
        "* *Possible improvements*\n",
        "\n",
        "    1.   Incorporating Stemming and Lemmatization\n",
        "    2.   Stopword Removal\n",
        "    3.   Enhancing Emoji Detection maybe with libraries that can perform this\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E6cwB_hicn45"
      }
    }
  ]
}